{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81419c04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81419c04",
    "outputId": "8ba1863f-c895-48fa-bf98-430a427a6e9b"
   },
   "outputs": [],
   "source": [
    "# Project: Energy Consumption Prediction\n",
    "# Phase 1: Project Setup and Initial Data Exploration\n",
    "\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set plot style for better aesthetics\n",
    "# 'fivethirtyeight' is a popular choice for data visualisation\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# 2. Load the Dataset\n",
    "try:\n",
    "    # df = pd.read_csv('data/PJME_hourly.csv')\n",
    "    df = pd.read_csv('sample_data/PJME_hourly.csv') # for Colab\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: PJME_hourly.csv not found. Make sure it is in the 'data/' directory.\")\n",
    "    print(\"Current working directory:\", os.getcwd()) # for debugging if there is a path issue\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst five rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Get the earliest and latest date and time in Datetime column\n",
    "print(\"\\nEarliest date and time:\", min(df['Datetime']))\n",
    "print(\"Latest date and time:\", max(df['Datetime']))\n",
    "\n",
    "# Get a summary of the DataFrame\n",
    "print(\"\\nDataFrame Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Get descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 3. Handling Missing Values (if any)\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values before processing:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e35f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "667e35f9",
    "outputId": "361d0658-6b1a-4012-8de5-19eef7d1f500"
   },
   "outputs": [],
   "source": [
    "# 4. Convert 'Datetime' Column to Datetime Object and Set as Index\n",
    "# Convert 'Datetime' column to datetime objects\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "# Set 'Datetime' as the DataFrame index and rename it\n",
    "df = df.set_index('Datetime').rename_axis('Datetime')\n",
    "\n",
    "# Ensure the index is sorted chronologically\n",
    "df = df.sort_index()\n",
    "\n",
    "# Check the new information to confirm datetime index\n",
    "print(\"\\nDataFrame Information after Datetime conversion and index setting:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for any duplicate timestamps\n",
    "# Should ideally be none for timestamps\n",
    "print(\"\\nNumber of duplicate timestamps:\", df.index.duplicated().sum())\n",
    "\n",
    "# Check for frequency consistency\n",
    "# If the data is truly hourly, the difference between consecutive timestamps should be 1 hour\n",
    "df.index.to_series().diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ab244",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e5ab244",
    "outputId": "7c3a69ac-9361-4f6d-ec23-8d4983a70139"
   },
   "outputs": [],
   "source": [
    "# Handle the 4 duplicate timestamps and the missing hours\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "\n",
    "# Handle duplicate timestamps\n",
    "# Keep the first occurrence for duplicates\n",
    "df = df.loc[~df.index.duplicated(keep='first')]\n",
    "print(\"Shape after removing duplicate timestamps:\", df.shape)\n",
    "\n",
    "# Create a complete, continuous hourly DateTimeIndex for the entire period\n",
    "# This fills in any missing hours such as those 2-hour gaps due to Daylight Saving Time transitions\n",
    "# Find the min and max datetime in the data\n",
    "start_date = df.index.min()\n",
    "end_date = df.index.max()\n",
    "\n",
    "# Generate a complete hourly date range\n",
    "full_date_range = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "\n",
    "# Reindex the DataFrame to this full date range\n",
    "# This inserts NaN for any missing hours\n",
    "df = df.reindex(full_date_range)\n",
    "print(\"Shape after reindexing to full hourly range:\", df.shape)\n",
    "\n",
    "# Handle missing values introduced by reindexing (due to two-hour gaps)\n",
    "# Fill in any actual missing hours that are NaN\n",
    "# For energy consumption, interpolating is often a good choice (especially linear or spline)\n",
    "print(\"\\nMissing values after reindexing (these are previously 'skipped' hours):\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Interpolate the missing values\n",
    "df['PJME_MW'] = df['PJME_MW'].interpolate(method='linear', limit_direction='both', limit_area='inside')\n",
    "\n",
    "print(\"\\nMissing values after interpolation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Verify the index frequency again (should be perfectly hourly now)\n",
    "print(\"\\nValue counts of item differences after full processing:\")\n",
    "print(df.index.to_series().diff().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06279ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06279ae9",
    "outputId": "1637811c-7963-41d7-9910-db93fd135d50"
   },
   "outputs": [],
   "source": [
    "# Phase 2: Feature Engineering\n",
    "\n",
    "# 1. Import Additional Library\n",
    "# Install the 'holidays' library using CLI if it is not done already\n",
    "# pip install holidays\n",
    "import holidays\n",
    "\n",
    "print(\"Additional feature engineering library imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed550c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fed550c",
    "outputId": "0c985659-e0c7-42ad-836e-4635e5914341"
   },
   "outputs": [],
   "source": [
    "# 2.1 Time-Based Features\n",
    "# These are features that are derived from the existing DatetimeIndex that help the model understand\n",
    "# hourly, daily, weekly, and annual seasonality.\n",
    "\n",
    "# Create features based on the datetime index\n",
    "df['hour_of_day'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek # 0 for Monday, 6 for Sunday\n",
    "df['day_of_year'] = df.index.dayofyear\n",
    "df['week_of_year'] = df.index.isocalendar().week.astype(int) # using isocalendar for week number\n",
    "df['month'] = df.index.month\n",
    "df['quarter'] = df.index.quarter\n",
    "df['year'] = df.index.year\n",
    "df['is_weekend'] = (df.index.dayofweek >= 5).astype(int) # 1 for weekend (Saturday, Sunday), 0 for weekday\n",
    "df['day_of_month'] = df.index.day # the day of the month (1-31)\n",
    "\n",
    "# Create cyclical features for hour, day of year, etc\n",
    "# These help the models that do not inherently understand cycles (such as linear models)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day'] / 24)\n",
    "df['dayofyear_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['dayofyear_cos'] = np.cos(2* np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "# Re-assert the index column name\n",
    "df.rename_axis('Datetime', inplace=True)\n",
    "\n",
    "print(\"DataFrame after adding time-based features (first 5 rows):\")\n",
    "print(df.head())\n",
    "print(\"\\nUnique values for a few of the new features:\")\n",
    "print(df['hour_of_day'].unique())\n",
    "print(df['day_of_week'].unique())\n",
    "print(df['is_weekend'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f7b26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c84f7b26",
    "outputId": "d330f2ad-4133-4dbe-a828-7d8508e9bf55"
   },
   "outputs": [],
   "source": [
    "# 2.2 Lagged Features\n",
    "# These features provide information about past energy consumption, which is highly indicative of future consumption.\n",
    "\n",
    "# Create lagged features\n",
    "# Consumption from the previous hour\n",
    "df['lag_1_hour'] = df['PJME_MW'].shift(1)\n",
    "\n",
    "# Consumption from the same hour on the previous day (24 hours before)\n",
    "df['lag_24_hour'] = df['PJME_MW'].shift(24)\n",
    "\n",
    "# Consumption from the same hour on the previous week (168 hours before)\n",
    "df['lag_168_hour'] = df['PJME_MW'].shift(168)\n",
    "\n",
    "# There are NaN values at the beginning of these shifted features, which are expected since there is no other data\n",
    "# to shift from the past. They will need to be handled before modelling by dropping or imputing them carefully.\n",
    "print(\"\\nDataFrame after adding lagged features (first few rows - will have NaNs):\")\n",
    "print(df.head(200))\n",
    "print(\"\\nNumber of NaNs in lagged features:\")\n",
    "print(df[['lag_1_hour', 'lag_24_hour', 'lag_168_hour']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9f161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdc9f161",
    "outputId": "77846029-befd-452d-b5a7-a8f93635a97c"
   },
   "outputs": [],
   "source": [
    "# 2.3 Public Holidays\n",
    "# Define the country and states for PJM (mostly US, specific states)\n",
    "# PJM covers all or parts of DE, IL, IN, KY, MD, MI, NC, NJ, OH, PA, TN, VA, WV, and Washington D.C.\n",
    "# Start with US federal holidays for simplicity.\n",
    "\n",
    "# Adjust the years to match the dataset's range of 2002 to 2018\n",
    "us_holidays = holidays.US(years=range(2002, 2019))\n",
    "\n",
    "# Create an 'is_holiday' column based on the DataFrame's index dates\n",
    "# The index only contains the date part for matching with the holidays library\n",
    "df['date_only'] = df.index.date\n",
    "df['is_holiday'] = df['date_only'].apply(lambda x: 1 if x in us_holidays else 0)\n",
    "\n",
    "# Drop the temporary 'date_only' column\n",
    "df = df.drop(columns=['date_only'])\n",
    "\n",
    "print(\"\\nDataFrame after adding 'is_holiday' feature (showing a few holiday entries):\")\n",
    "print(df[df['is_holiday'] == 1].head())\n",
    "print(\"\\nNumber of holidays found:\", df['is_holiday'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad83f38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fad83f38",
    "outputId": "8f876eb0-e450-4cd5-9ed2-9b8073d7586f"
   },
   "outputs": [],
   "source": [
    "# 2.4 Weather Data (Temperature)\n",
    "# Use historical hourly temperature data for a representative location within the PJM service territory.\n",
    "# Data is retrieved from Open-Meteo by using their API\n",
    "# Selected region: Chicago\n",
    "# Latitude: 41.8832, longitude: -87.6324\n",
    "\n",
    "# Retrieve historical temperature data from Open-Meteo\n",
    "\n",
    "import requests\n",
    "\n",
    "# Define the coordinates for Chicago\n",
    "latitude = 41.8832\n",
    "longitude = -87.6324\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2002-01-01'\n",
    "end_date = '2018-08-03'\n",
    "\n",
    "# Open-Meteo Historical Weather API endpoint\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"hourly\": \"temperature_2m\", # request hourly temperature at 2 meters\n",
    "    \"timezone\": \"America/Chicago\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Ensure the energy consumption data has an index name\n",
    "df.index.name = 'Datetime'\n",
    "\n",
    "# Check if the API call was successful\n",
    "if 'hourly' in data:\n",
    "    temp_df = pd.DataFrame(data['hourly'])\n",
    "    temp_df['time'] = pd.to_datetime(temp_df['time'])\n",
    "    temp_df = temp_df.set_index('time')\n",
    "    temp_df.index.name = 'Datetime'\n",
    "    temp_df = temp_df.rename(columns={'temperature_2m': 'current_temp'}) # rename column for temperature at 2 meters\n",
    "    print(\"Temperature data loaded successfully.\")\n",
    "    print(temp_df.head())\n",
    "    print(temp_df.info())\n",
    "else:\n",
    "    print(\"Error fetching temperature data:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603d8d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7603d8d8",
    "outputId": "4da51594-f72a-4995-ed5c-d2df23609b17"
   },
   "outputs": [],
   "source": [
    "# Merge temp_df with the main energy consumption DataFrame (df)\n",
    "df = df.merge(temp_df[['current_temp']], left_index=True, right_index=True, how='left')\n",
    "df.rename(columns={'current_temp': 'temperature'}, inplace=True)\n",
    "\n",
    "print(\"Merged df (first 5 rows):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c86ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea1c86ac",
    "outputId": "84fe9066-c60d-4a41-c094-583b13adce49"
   },
   "outputs": [],
   "source": [
    "# Create temperature-derived features\n",
    "\n",
    "# Lagged temperature feature\n",
    "df['lag_24_temp'] = df['temperature'].shift(24)\n",
    "\n",
    "# 3-day rolling average temperature\n",
    "df['rolling_72_temp_avg'] = df['temperature'].rolling(window=72).mean() # 3-day rolling average\n",
    "\n",
    "# Squared temperature terms\n",
    "df['temp_squared'] = np.power(df['temperature'], 2)\n",
    "\n",
    "# There are NaN values at the beginning of these shifted features, which are expected since there is no other data\n",
    "# to shift from the past. They will need to be handled before modelling by dropping or imputing them carefully.\n",
    "print(\"\\nDataFrame after adding temperature-derived features (first few rows - will have NaNs):\")\n",
    "print(df.head(200))\n",
    "print(\"\\nNumber of NaNs in temperature_derived features:\")\n",
    "print(df[['lag_24_temp', 'rolling_72_temp_avg', 'temp_squared']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5544c4d-50c8-4132-b3cb-e5ef35a7f27c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5544c4d-50c8-4132-b3cb-e5ef35a7f27c",
    "outputId": "51a502ac-0de4-41ef-a040-e0c3fc624ac2"
   },
   "outputs": [],
   "source": [
    "# Phase 3: Advanced Feature Engineering\n",
    "\n",
    "# 1.1 Derived Weather Features (Heating and Cooling Degree Days)\n",
    "# Capture more complex, non-linear and domain-specific relationships between weather conditions and\n",
    "# energy consumption as compared to using raw temperature data alone.\n",
    "# The base temperature used is 18Â°C.\n",
    "\n",
    "# Check if 'Datetime' column exists. Set it if it is not the index already\n",
    "if 'Datetime' in df.columns:\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df = df.set_index('Datetime')\n",
    "# Convert it if 'Datetime' is already the index but its dtype isn't datetime64[ns]\n",
    "elif not isinstance(df.index, pd.DatetimeIndex):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Define a base temperature\n",
    "# 18 degrees Celsius is common for energy forecasting\n",
    "base_temperature_celsius = 18\n",
    "\n",
    "# Calculate daily average temperature\n",
    "# Resample will work correctly now that we're sure df.index is a DatetimeIndex\n",
    "daily_temp_avg = df['temperature'].resample('D').mean()\n",
    "\n",
    "# Calculate HDD and CDD\n",
    "df_daily_hdd_cdd = pd.DataFrame(index=daily_temp_avg.index)\n",
    "df_daily_hdd_cdd['daily_avg_temp'] = daily_temp_avg\n",
    "\n",
    "df_daily_hdd_cdd['hdd'] = (base_temperature_celsius - df_daily_hdd_cdd['daily_avg_temp']).apply(lambda x: max(0, x))\n",
    "df_daily_hdd_cdd['cdd'] = (df_daily_hdd_cdd['daily_avg_temp'] - base_temperature_celsius).apply(lambda x: max(0, x))\n",
    "\n",
    "# Map HDD and CDD back to your original hourly DataFrame\n",
    "# Use .normalize() on the hourly index to match the daily index for mapping\n",
    "df['hdd'] = df.index.normalize().map(df_daily_hdd_cdd['hdd'])\n",
    "df['cdd'] = df.index.normalize().map(df_daily_hdd_cdd['cdd'])\n",
    "\n",
    "# Handle potential NaNs such as if there were gaps or mismatches\n",
    "# Use explicit assignment to avoid the FutureWarning\n",
    "df['hdd'] = df['hdd'].fillna(0)\n",
    "df['cdd'] = df['cdd'].fillna(0)\n",
    "\n",
    "# Inspect the new features\n",
    "print(df[['temperature', 'hdd', 'cdd']].head())\n",
    "print(df[['temperature', 'hdd', 'cdd']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5571416-2ffa-4650-9e08-15f6d2c7cf34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5571416-2ffa-4650-9e08-15f6d2c7cf34",
    "outputId": "1fc23eef-0fc1-4dbd-a4a4-ade56f4ead8c"
   },
   "outputs": [],
   "source": [
    "# 1.2 Rolling Window Statistics\n",
    "# Capture recent trends and volatility of the target variable, `PJME_MW` over specific time windows to\n",
    "# give the model valuable context about the immediate past\n",
    "\n",
    "# 24-hour (1-day) Rolling Mean of PJME_MW\n",
    "# .rolling(window='24H') creates a rolling window of 24 hours.\n",
    "# .mean() calculates the mean within that window.\n",
    "# .bfill() is used here to fill NaN values at the beginning of the series, where there aren't enough\n",
    "# previous data points to fill the window.\n",
    "df['PJME_MW_rolling_24_hr_mean'] = df['PJME_MW'].rolling(window='24h', closed='left').mean().bfill()\n",
    "\n",
    "# 168-hour (7-day) Rolling Mean of PJME_MW\n",
    "df['PJME_MW_rolling_168_hr_mean'] = df['PJME_MW'].rolling(window='168h', closed='left').mean().bfill()\n",
    "\n",
    "# 24-hour (1-day) Rolling Standard Deviation of PJME_MW\n",
    "df['PJME_MW_rolling_24_hr_std'] = df['PJME_MW'].rolling(window='24h', closed='left').std().bfill()\n",
    "\n",
    "# 168-hour (7-day) Rolling Standard Deviation of PJME_MW\n",
    "df['PJME_MW_rolling_168_hr_std'] = df['PJME_MW'].rolling(window='168h', closed='left').std().bfill()\n",
    "\n",
    "# Inspect the new features\n",
    "print(\"\\n--- After adding Rolling Mean Features ---\")\n",
    "print(df[['PJME_MW', 'PJME_MW_rolling_24_hr_mean', 'PJME_MW_rolling_168_hr_mean']].head(40)) # show more rows to see rolling effect\n",
    "print(df[['PJME_MW', 'PJME_MW_rolling_24_hr_mean', 'PJME_MW_rolling_168_hr_mean']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27901b2c-a047-4562-97e9-459640fa4a23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27901b2c-a047-4562-97e9-459640fa4a23",
    "outputId": "e85641bb-b2c2-4fea-e0c6-b7da3bec7b03"
   },
   "outputs": [],
   "source": [
    "# 1.3 Interaction Features\n",
    "# Created by combining two or more existing features to help the model capture relationships that are\n",
    "# less apparent from individual features alone.\n",
    "\n",
    "# Hour of day interacted with `is_weekend`\n",
    "# Creates a unique value for each hour on a weekend vs on a weekday\n",
    "# 1 is added to is_weekend (0 or 1) to distinguish weekend hours.\n",
    "df['hour_of_day_x_is_weekend'] = df['hour_of_day'] * (df['is_weekend'] + 1)\n",
    "\n",
    "# Temperature interacted with `is_holiday`\n",
    "# Highlights temperature effects specifically on holidays\n",
    "# 1 is added to is_holiday (0 or 1) to distinguish holiday temperatures.\n",
    "df['temperature_x_is_holiday'] = df['temperature'] * (df['is_holiday'] + 1)\n",
    "\n",
    "# Temperature interacted with `hour_of_day`\n",
    "# Accounts for the varying effect of temperature on demand at different hours of the day\n",
    "df['temperature_x_hour_of_day'] = df['temperature'] * (df['hour_of_day'] + 1)\n",
    "\n",
    "# CDD interacted with `is_weekend`\n",
    "# Captures the potentially different impact of cooling degree days (CDD) on weekends\n",
    "# as compared to weekdays\n",
    "df['cdd_x_is_weekend'] = df['cdd'] * df['is_weekend']\n",
    "\n",
    "# Inspect the new features\n",
    "print(\"\\n--- After adding Interaction Features ---\")\n",
    "print(df[['hour_of_day', 'is_weekend', 'hour_of_day_x_is_weekend',\n",
    "          'temperature', 'is_holiday', 'temperature_x_is_holiday']].head())\n",
    "print(df[['hour_of_day', 'is_weekend', 'hour_of_day_x_is_weekend',\n",
    "          'temperature', 'is_holiday', 'temperature_x_is_holiday']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd461da9-722b-4b02-9566-f7b73681337f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd461da9-722b-4b02-9566-f7b73681337f",
    "outputId": "a114bb34-4bab-411c-a34b-aaa61c3719e9"
   },
   "outputs": [],
   "source": [
    "print(\"Columns in df BEFORE data splitting:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac11105",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bac11105",
    "outputId": "ca05b811-8cea-4dfe-e9d4-786be14e3f47"
   },
   "outputs": [],
   "source": [
    "# Drop the NaN values\n",
    "# Given that the dataset covers years of hourly data from 2002 to 2018, which is over 140,000 hours, the maximum number\n",
    "# of NaN values is very small in relative to the entire dataset size (0.12%), so it is acceptable to drop the rows with\n",
    "# NaN values for time series data.\n",
    "\n",
    "# Create a list of all feature columns that may contain NaN values\n",
    "feature_columns_to_check = [\n",
    "    'lag_1_hour', 'lag_24_hour', 'lag_168_hour', 'lag_24_temp', 'rolling_72_temp_avg', 'temp_squared'\n",
    "]\n",
    "\n",
    "# Drop rows where any of these specified feature columns have NaN values\n",
    "df.dropna(subset=feature_columns_to_check, inplace=True)\n",
    "\n",
    "# Verify that the rows with NaN values are gone\n",
    "print(\"Number of NaNs after dropping:\")\n",
    "print(df[feature_columns_to_check].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ec27f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "500ec27f",
    "outputId": "3069b519-a5f1-43a2-8444-96caa167b1f8"
   },
   "outputs": [],
   "source": [
    "# Phase 4: Model Selection and Training\n",
    "# 1. Data Splitting and Feature/Target Definition\n",
    "# A chronological split with a cutoff date is the approach adopted to prepare the dataset for model training and\n",
    "# evaluation. This method is critical for time series data to prevent data leakage, which would otherwise occur with\n",
    "# random splitting. Data leakage exposes the model to \"future information\" during training, leading to overly optimistic\n",
    "# performance estimates on the test set.\n",
    "# For this project, our choice of cutoff date for the test set is 01 January 2017, which roughly encompasses the last\n",
    "# 1.5 years of the dataset.\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = '2017-01-01'\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "# The target variable is 'PJME_MW'\n",
    "target = 'PJME_MW'\n",
    "\n",
    "# List out all final features excluding the target variable\n",
    "# Ensure that only the intended features are used and makes the model's inputs very clear for everyone\n",
    "exclude_features = ['is_weekend_label']\n",
    "features = [col for col in df.columns if col != target and col not in exclude_features]\n",
    "\n",
    "print(\"\\nList of all features:\")\n",
    "print(features)\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split the data chronologically using the explicit feature list\n",
    "X_train = df[df.index < cutoff_date][features]\n",
    "y_train = df[df.index < cutoff_date][target]\n",
    "\n",
    "X_test = df[df.index >= cutoff_date][features]\n",
    "y_test = df[df.index >= cutoff_date][target]\n",
    "\n",
    "print(f\"Full data shape: {df.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTrain data range: {X_train.index.min()} to {X_train.index.max()}\")\n",
    "print(f\"Test data range: {X_test.index.min()} to {X_test.index.max()}\")\n",
    "\n",
    "# Verify that there is no overlap\n",
    "if X_train.index.max() < X_test.index.min():\n",
    "    print(\"\\nData split verified: No temporal overlap between train and test sets.\")\n",
    "else:\n",
    "    print(\"\\nWarning: Temporal overlap detected. Review your cutoff date or splitting logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bc241-10bd-467a-8900-26b534ae6d61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "250bc241-10bd-467a-8900-26b534ae6d61",
    "outputId": "91e50579-b500-40e9-fd85-b56f64cbe610"
   },
   "outputs": [],
   "source": [
    "print(\"\\nColumns in X_test AFTER data splitting:\")\n",
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r8zVX1Y7B2L3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "r8zVX1Y7B2L3",
    "outputId": "2e304c36-f919-461c-acfb-a14147073114"
   },
   "outputs": [],
   "source": [
    "# Phase 5: Hyperparameter Optimisation using Optuna\n",
    "\n",
    "# 1. Load the Saved Files of the Tuned XGBoost and LightGBM Models\n",
    "\n",
    "# Upload files in Google Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify the upload\n",
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40981684-1245-4ced-a97f-d6c198a4334e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40981684-1245-4ced-a97f-d6c198a4334e",
    "outputId": "6b5ef7c7-9df6-4e33-f77a-9bcfe0244321"
   },
   "outputs": [],
   "source": [
    "# Import additional libraries\n",
    "!pip install xgboost lightgbm  tensorflow\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Load the saved tuned models\n",
    "print(\"Loading both tuned XGBoost and LightGBM models from file...\")\n",
    "try:\n",
    "  tuned_xgb_model = joblib.load('tuned_xgboost_model.joblib')\n",
    "  tuned_lgbm_model = joblib.load('tuned_lightgbm_model.joblib')\n",
    "  print(\"Tuned models loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "  print(\"Error: Model files not found. Please ensure that both 'tuned_xgboost_model' and 'tuned_lightgbm_model' are uploaded to your Colab session.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AUeA9x79SWoD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUeA9x79SWoD",
    "outputId": "de574302-ff0d-4a1b-ab6c-f75f050d708b"
   },
   "outputs": [],
   "source": [
    "# Phase 6: Advanced Model Exploration\n",
    "\n",
    "# 1. Advanced Exploration for the Tuned XGBoost Model\n",
    "# Improves the performance of the best XGBoost model by using a simple stacking approach.\n",
    "\n",
    "# --- Stacking Ensemble ---\n",
    "\n",
    "# Generate predictions from the tuned model on the test set\n",
    "# These predictions will be the single feature for our meta-model.\n",
    "print(\"Generating predictions from the tuned XGBoost model on the test set...\")\n",
    "xgb_preds = tuned_xgb_model.predict(X_test)\n",
    "\n",
    "# Reshape the predictions for the meta-model\n",
    "# As LinearRegression expects a 2D array, it needs to be reshaped.\n",
    "xgb_stacked_feature = xgb_preds.reshape(-1, 1)\n",
    "\n",
    "# Train a simple Linear Regression meta-model\n",
    "print(\"Training a Linear Regression meta-model to adjust XGBoost predictions...\")\n",
    "xgb_meta_model = LinearRegression()\n",
    "xgb_meta_model.fit(xgb_stacked_feature, y_test)\n",
    "\n",
    "# Generate the final predictions from the stacking model\n",
    "xgb_final_predictions = xgb_meta_model.predict(xgb_stacked_feature)\n",
    "\n",
    "# Evaluate the performance of the stacking model\n",
    "xgb_stacked_rmse = np.sqrt(mean_squared_error(y_test, xgb_final_predictions))\n",
    "xgb_stacked_mae = mean_absolute_error(y_test, xgb_final_predictions)\n",
    "xgb_stacked_r2 = r2_score(y_test, xgb_final_predictions)\n",
    "\n",
    "print(\"\\n--- Stacking Ensemble Model Performance for XGBoost ---\")\n",
    "print(f\"RMSE: {xgb_stacked_rmse:.4f}\")\n",
    "print(f\"MAE: {xgb_stacked_mae:.4f}\")\n",
    "print(f\"R-squared: {xgb_stacked_r2:.4f}\")\n",
    "\n",
    "# Save the meta-model for future use\n",
    "joblib.dump(xgb_meta_model, 'xgboost_meta_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W7cFlRqlYmN_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7cFlRqlYmN_",
    "outputId": "5dea0b57-d341-4da5-93c1-0a91a6787fb2"
   },
   "outputs": [],
   "source": [
    "# 2. Advanced Exploration for the Tuned LightGBM Model\n",
    "# Improves the performance of the best LightGBM model by using a simple stacking approach.\n",
    "\n",
    "# --- Stacking Ensemble ---\n",
    "\n",
    "# Generate predictions from the tuned model on the test set\n",
    "# These predictions will be the single feature for our meta-model.\n",
    "print(\"Generating predictions from the tuned LightGBM model on the test set...\")\n",
    "lgbm_preds = tuned_lgbm_model.predict(X_test)\n",
    "\n",
    "# Reshape the predictions for the meta-model\n",
    "# As LinearRegression expects a 2D array, it needs to be reshaped.\n",
    "lgbm_stacked_feature = lgbm_preds.reshape(-1, 1)\n",
    "\n",
    "# Train a simple Linear Regression meta-model\n",
    "print(\"Training a Linear Regression meta-model to adjust LightGBM predictions...\")\n",
    "lgbm_meta_model = LinearRegression()\n",
    "lgbm_meta_model.fit(lgbm_stacked_feature, y_test)\n",
    "\n",
    "# Generate the final predictions from the stacking model\n",
    "lgbm_final_predictions = lgbm_meta_model.predict(lgbm_stacked_feature)\n",
    "\n",
    "# Evaluate the performance of the stacking model\n",
    "lgbm_stacked_rmse = np.sqrt(mean_squared_error(y_test, lgbm_final_predictions))\n",
    "lgbm_stacked_mae = mean_absolute_error(y_test, lgbm_final_predictions)\n",
    "lgbm_stacked_r2 = r2_score(y_test, lgbm_final_predictions)\n",
    "\n",
    "print(\"\\n--- Stacking Ensemble Model Performance for LightGBM ---\")\n",
    "print(f\"RMSE: {lgbm_stacked_rmse:.4f}\")\n",
    "print(f\"MAE: {lgbm_stacked_mae:.4f}\")\n",
    "print(f\"R-squared: {lgbm_stacked_r2:.4f}\")\n",
    "\n",
    "# Save the meta-model for future use\n",
    "joblib.dump(lgbm_meta_model, 'lightgbm_meta_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2_ZwAZ8A_NI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2_ZwAZ8A_NI",
    "outputId": "2f895b4c-21e3-4fd5-d48c-d5d2dcb1d1fa"
   },
   "outputs": [],
   "source": [
    "# 3. Multi-Model Ensemble\n",
    "# Combines both tuned XGBoost and LightGBM models into a final, powerful stacking ensemble.\n",
    "\n",
    "# --- Stacking Ensemble ---\n",
    "\n",
    "# Generate the predictions of both tuned models on the test set\n",
    "# These predictions will be the single feature for the meta-model.\n",
    "\n",
    "print(\"\\nGenerating predictions from the tuned XGBoost model on the test set...\")\n",
    "xgb_preds = tuned_xgb_model.predict(X_test)\n",
    "print(\"Generating predictions from the tuned LightGBM model on the test set...\")\n",
    "lgbm_preds = tuned_lgbm_model.predict(X_test)\n",
    "\n",
    "# Combine the predictions from both models into a new DataFrame\n",
    "stacked_features = pd.DataFrame({\n",
    "    'xgb_preds': xgb_preds,\n",
    "    'lgbm_preds': lgbm_preds\n",
    "})\n",
    "\n",
    "# Train a new Linear Regression meta-model on the combined predictions\n",
    "print(\"Training a new Linear Regression model meta-model on the combined predictions...\")\n",
    "final_meta_model = LinearRegression()\n",
    "final_meta_model.fit(stacked_features, y_test)\n",
    "\n",
    "# Generate the final predictions from the advanced stacking model\n",
    "final_predictions = final_meta_model.predict(stacked_features)\n",
    "\n",
    "# Evaluate the performance of the final stacking model\n",
    "final_stacked_rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
    "final_stacked_mae = mean_absolute_error(y_test, final_predictions)\n",
    "final_stacked_r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "print(\"\\n--- Advanced Stacking Ensemble Model Performance for XGBoost and LightGBM ---\")\n",
    "print(f\"RMSE: {final_stacked_rmse:.4f}\")\n",
    "print(f\"MAE: {final_stacked_mae:.4f}\")\n",
    "print(f\"R-squared: {final_stacked_r2:.4f}\")\n",
    "\n",
    "# Save the final meta-model for future use\n",
    "joblib.dump(final_meta_model, 'advanced_stacking_meta_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TCYfysT8GQAS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCYfysT8GQAS",
    "outputId": "1ae30192-c24d-410a-e34c-f83375771975"
   },
   "outputs": [],
   "source": [
    "# 4. Compare the performance of all three models\n",
    "\n",
    "# Calculate the performance metrics for the individual XGBoost and LightGBM models\n",
    "# Facilitates a direct comparison with the stacked model.\n",
    "\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_preds))\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_preds)\n",
    "xgb_r2 = r2_score(y_test, xgb_preds)\n",
    "\n",
    "lgbm_rmse = np.sqrt(mean_squared_error(y_test, lgbm_preds))\n",
    "lgbm_mae = mean_absolute_error(y_test, lgbm_preds)\n",
    "lgbm_r2 = r2_score(y_test, lgbm_preds)\n",
    "\n",
    "# Calculate the performance metrics for the final stacking ensemble\n",
    "final_stacked_rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
    "final_stacked_mae = mean_absolute_error(y_test, final_predictions)\n",
    "final_stacked_r2 = r2_score(y_test, final_predictions)\n",
    "\n",
    "# Create a summary DataFrame for easy comparison\n",
    "performance_summary = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM', 'Advanced Stacking Ensemble'],\n",
    "    'RMSE': [xgb_rmse, lgbm_rmse, final_stacked_rmse],\n",
    "    'MAE': [xgb_mae, lgbm_mae, final_stacked_mae],\n",
    "    'R-squared': [xgb_r2, lgbm_r2, final_stacked_r2]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Final Model Performance Summary ---\")\n",
    "print(performance_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hhGo3I-JMZbP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hhGo3I-JMZbP",
    "outputId": "a9e3e4a9-2326-4155-b308-9516ceaa4f70"
   },
   "outputs": [],
   "source": [
    "# 5. Visualise the Comparison Results\n",
    "\n",
    "# Create the 'images' directory if it does not exist yet\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "# Set a style for the plots\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Plot the actual values vs the predictions from the best model\n",
    "# The final stacking ensemble is expected to have the best results.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(y_test.index, y_test,\n",
    "        label='Actual Values', color='brown',\n",
    "        linewidth=3)\n",
    "ax.plot(y_test.index, final_predictions,\n",
    "        label='Stacking Ensemble Predictions', color='orchid',\n",
    "        linewidth=2)\n",
    "ax.set_title('Actual vs Predicted PJME_MW (Test Set)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('PJME_MW')\n",
    "ax.legend()\n",
    "plt.savefig('images/ph6_actual_vs_predicted_final_model.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# Plot the distribution of residuals\n",
    "# A good model would have residuals centered around zero.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "residuals = y_test - final_predictions\n",
    "sns.histplot(residuals, bins=50, kde=True,lag_ ax=ax, color='green')\n",
    "ax.set_title('Distribution of Residuals')\n",
    "ax.set_xlabel('Residuals (Actual - Predicted)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "plt.savefig('images/ph6_residual_distribution_final_model.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3sTGrWD2L5TL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3sTGrWD2L5TL",
    "outputId": "0cb6e5b0-72b2-4489-f23a-5f1dc531d6a8"
   },
   "outputs": [],
   "source": [
    "# Phase 7: Model Deployment using Streamlit Application (app.py)\n",
    "\n",
    "# 1. Load Additional Library\n",
    "import joblib\n",
    "\n",
    "# 2. Create the joblib file for loading into the app\n",
    "# Create a dictionary to hold all trained models\n",
    "pipeline_models = {\n",
    "    'xgb_model': tuned_xgb_model,\n",
    "    'lgbm_model': tuned_lgbm_model,\n",
    "    'meta_model': final_meta_model\n",
    "}\n",
    "\n",
    "# Save the dictionary as a single .joblib file\n",
    "joblib.dump(pipeline_models, 'production_pipeline.joblib')\n",
    "\n",
    "print('All models saved to production_pipeline.joblib.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yI5eu6Nh8BGt",
   "metadata": {
    "id": "yI5eu6Nh8BGt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ML Projects)",
   "language": "python",
   "name": "ml-projects-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
