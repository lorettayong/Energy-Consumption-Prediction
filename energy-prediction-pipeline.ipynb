{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81419c04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81419c04",
    "outputId": "5e62296b-6433-49f7-8a47-9232d0ae9447"
   },
   "outputs": [],
   "source": [
    "# Project: Energy Consumption Prediction\n",
    "# Phase 1: Project Setup and Initial Data Exploration\n",
    "\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set plot style for better aesthetics\n",
    "# 'fivethirtyeight' is a popular choice for data visualisation\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# 2. Load the Dataset\n",
    "try:\n",
    "    df = pd.read_csv('data/PJME_hourly.csv')\n",
    "    # df = pd.read_csv('sample_data/PJME_hourly.csv') # for Colab\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: PJME_hourly.csv not found. Make sure it is in the 'data/' directory.\")\n",
    "    print(\"Current working directory:\", os.getcwd()) # for debugging if there is a path issue\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nFirst five rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Get the earliest and latest date and time in Datetime column\n",
    "print(\"\\nEarliest date and time:\", min(df['Datetime']))\n",
    "print(\"Latest date and time:\", max(df['Datetime']))\n",
    "\n",
    "# Get a summary of the DataFrame\n",
    "print(\"\\nDataFrame Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Get descriptive statistics for numerical columns\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# 3. Handling Missing Values (if any)\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values before processing:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e35f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "667e35f9",
    "outputId": "abce230c-b135-45c1-b517-4c7a8969d379"
   },
   "outputs": [],
   "source": [
    "# 4. Convert 'Datetime' Column to Datetime Object and Set as Index\n",
    "# Convert 'Datetime' column to datetime objects\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "# Set 'Datetime' as the DataFrame index and rename it\n",
    "df = df.set_index('Datetime').rename_axis('Datetime')\n",
    "\n",
    "# Ensure the index is sorted chronologically\n",
    "df = df.sort_index()\n",
    "\n",
    "# Check the new information to confirm datetime index\n",
    "print(\"\\nDataFrame Information after Datetime conversion and index setting:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for any duplicate timestamps\n",
    "# Should ideally be none for timestamps\n",
    "print(\"\\nNumber of duplicate timestamps:\", df.index.duplicated().sum())\n",
    "\n",
    "# Check for frequency consistency\n",
    "# If the data is truly hourly, the difference between consecutive timestamps should be 1 hour\n",
    "df.index.to_series().diff().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ab244",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e5ab244",
    "outputId": "bb8e87bc-08be-4392-a6e4-2948c2c20e56"
   },
   "outputs": [],
   "source": [
    "# Handle the 4 duplicate timestamps and the missing hours\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "\n",
    "# Handle duplicate timestamps\n",
    "# Keep the first occurrence for duplicates\n",
    "df = df.loc[~df.index.duplicated(keep='first')]\n",
    "print(\"Shape after removing duplicate timestamps:\", df.shape)\n",
    "\n",
    "# Create a complete, continuous hourly DateTimeIndex for the entire period\n",
    "# This fills in any missing hours such as those 2-hour gaps due to Daylight Saving Time transitions\n",
    "# Find the min and max datetime in the data\n",
    "start_date = df.index.min()\n",
    "end_date = df.index.max()\n",
    "\n",
    "# Generate a complete hourly date range\n",
    "full_date_range = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "\n",
    "# Reindex the DataFrame to this full date range\n",
    "# This inserts NaN for any missing hours\n",
    "df = df.reindex(full_date_range)\n",
    "print(\"Shape after reindexing to full hourly range:\", df.shape)\n",
    "\n",
    "# Handle missing values introduced by reindexing (due to two-hour gaps)\n",
    "# Fill in any actual missing hours that are NaN\n",
    "# For energy consumption, interpolating is often a good choice (especially linear or spline)\n",
    "print(\"\\nMissing values after reindexing (these are previously 'skipped' hours):\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Interpolate the missing values\n",
    "df['PJME_MW'] = df['PJME_MW'].interpolate(method='linear', limit_direction='both', limit_area='inside')\n",
    "\n",
    "print(\"\\nMissing values after interpolation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Verify the index frequency again (should be perfectly hourly now)\n",
    "print(\"\\nValue counts of item differences after full processing:\")\n",
    "print(df.index.to_series().diff().value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a487f46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3a487f46",
    "outputId": "7c50e27f-8212-4768-feba-db62383198a1"
   },
   "outputs": [],
   "source": [
    "# 5. Initial Visualisations using Exploratory Data Analysis (EDA)\n",
    "\n",
    "# Create the 'images' directory if it does not exist yet\n",
    "if not os.path.exists('images'):\n",
    "    os.makedirs('images')\n",
    "\n",
    "# Plot the entire time series\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "df['PJME_MW'].plot(ax=ax, title='PJME Hourly Energy Consumption (MW)')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Consumption (MW)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph1_entire_time_series.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# Zoom in on a specific period to see daily/weekly patterns\n",
    "# Example: May 2015\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "df.loc['2015-03'].plot(ax=ax, title='PJME Hourly Energy Consumption in March 2015')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Consumption (MW)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph1_zoom_period_time_series.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# Further zoom in: A single week in 2015\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "df.loc['2015-03-01':'2015-03-07'].plot(ax=ax, title='PJME Hourly Energy Consumption in the First Week of March 2015')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Consumption (MW)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph1_zoom_week_time_series.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06279ae9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06279ae9",
    "outputId": "0e56b78c-c257-42b7-c119-431f975a47d3"
   },
   "outputs": [],
   "source": [
    "# Phase 2: Feature Engineering\n",
    "\n",
    "# 1. Import Additional Library\n",
    "# Install the 'holidays' library using CLI if it is not done already\n",
    "# pip install holidays\n",
    "import holidays\n",
    "\n",
    "print(\"Additional feature engineering library imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed550c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fed550c",
    "outputId": "af4de463-964e-4d6f-e067-5e2016e428bb"
   },
   "outputs": [],
   "source": [
    "# 2.1 Time-Based Features\n",
    "# These are features that are derived from the existing DatetimeIndex that help the model understand\n",
    "# hourly, daily, weekly, and annual seasonality.\n",
    "\n",
    "# Create features based on the datetime index\n",
    "df['hour_of_day'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek # 0 for Monday, 6 for Sunday\n",
    "df['day_of_year'] = df.index.dayofyear\n",
    "df['week_of_year'] = df.index.isocalendar().week.astype(int) # using isocalendar for week number\n",
    "df['month'] = df.index.month\n",
    "df['quarter'] = df.index.quarter\n",
    "df['year'] = df.index.year\n",
    "df['is_weekend'] = (df.index.dayofweek >= 5).astype(int) # 1 for weekend (Saturday, Sunday), 0 for weekday\n",
    "df['day_of_month'] = df.index.day # the day of the month (1-31)\n",
    "\n",
    "# Create cyclical features for hour, day of year, etc\n",
    "# These help the models that do not inherently understand cycles (such as linear models)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day'] / 24)\n",
    "df['dayofyear_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['dayofyear_cos'] = np.cos(2* np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "# Re-assert the index column name\n",
    "df.rename_axis('Datetime', inplace=True)\n",
    "\n",
    "print(\"DataFrame after adding time-based features (first 5 rows):\")\n",
    "print(df.head())\n",
    "print(\"\\nUnique values for a few of the new features:\")\n",
    "print(df['hour_of_day'].unique())\n",
    "print(df['day_of_week'].unique())\n",
    "print(df['is_weekend'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f7b26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c84f7b26",
    "outputId": "ad5bd600-3597-4244-cb23-b3c8a7f7cb34"
   },
   "outputs": [],
   "source": [
    "# 2.2 Lagged Features\n",
    "# These features provide information about past energy consumption, which is highly indicative of future consumption.\n",
    "\n",
    "# Create lagged features\n",
    "# Consumption from the previous hour\n",
    "df['lag_1_hour'] = df['PJME_MW'].shift(1)\n",
    "\n",
    "# Consumption from the same hour on the previous day (24 hours before)\n",
    "df['lag_24_hour'] = df['PJME_MW'].shift(24)\n",
    "\n",
    "# Consumption from the same hour on the previous week (168 hours before)\n",
    "df['lag_168_hour'] = df['PJME_MW'].shift(168)\n",
    "\n",
    "# There are NaN values at the beginning of these shifted features, which are expected since there is no other data\n",
    "# to shift from the past. They will need to be handled before modelling by dropping or imputing them carefully.\n",
    "print(\"\\nDataFrame after adding lagged features (first few rows - will have NaNs):\")\n",
    "print(df.head(200))\n",
    "print(\"\\nNumber of NaNs in lagged features:\")\n",
    "print(df[['lag_1_hour', 'lag_24_hour', 'lag_168_hour']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9f161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdc9f161",
    "outputId": "7e63cd0d-1183-49d2-ba0e-2a6783ba8b04"
   },
   "outputs": [],
   "source": [
    "# 2.3 Public Holidays\n",
    "# Define the country and states for PJM (mostly US, specific states)\n",
    "# PJM covers all or parts of DE, IL, IN, KY, MD, MI, NC, NJ, OH, PA, TN, VA, WV, and Washington D.C.\n",
    "# Start with US federal holidays for simplicity.\n",
    "\n",
    "# Adjust the years to match the dataset's range of 2002 to 2018\n",
    "us_holidays = holidays.US(years=range(2002, 2019))\n",
    "\n",
    "# Create an 'is_holiday' column based on the DataFrame's index dates\n",
    "# The index only contains the date part for matching with the holidays library\n",
    "df['date_only'] = df.index.date\n",
    "df['is_holiday'] = df['date_only'].apply(lambda x: 1 if x in us_holidays else 0)\n",
    "\n",
    "# Drop the temporary 'date_only' column\n",
    "df = df.drop(columns=['date_only'])\n",
    "\n",
    "print(\"\\nDataFrame after adding 'is_holiday' feature (showing a few holiday entries):\")\n",
    "print(df[df['is_holiday'] == 1].head())\n",
    "print(\"\\nNumber of holidays found:\", df['is_holiday'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad83f38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fad83f38",
    "outputId": "13360c65-3e95-4426-9aaa-342478c7a1ca"
   },
   "outputs": [],
   "source": [
    "# 2.4 Weather Data (Temperature)\n",
    "# Use historical hourly temperature data for a representative location within the PJM service territory.\n",
    "# Data is retrieved from Open-Meteo by using their API\n",
    "# Selected region: Chicago\n",
    "# Latitude: 41.8832, longitude: -87.6324\n",
    "\n",
    "# Retrieve historical temperature data from Open-Meteo\n",
    "\n",
    "import requests\n",
    "\n",
    "# Define the coordinates for Chicago\n",
    "latitude = 41.8832\n",
    "longitude = -87.6324\n",
    "\n",
    "# Define the date range\n",
    "start_date = '2002-01-01'\n",
    "end_date = '2018-08-03'\n",
    "\n",
    "# Open-Meteo Historical Weather API endpoint\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"hourly\": \"temperature_2m\", # request hourly temperature at 2 meters\n",
    "    \"timezone\": \"America/Chicago\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "data = response.json()\n",
    "\n",
    "# Ensure the energy consumption data has an index name\n",
    "df.index.name = 'Datetime'\n",
    "\n",
    "# Check if the API call was successful\n",
    "if 'hourly' in data:\n",
    "    temp_df = pd.DataFrame(data['hourly'])\n",
    "    temp_df['time'] = pd.to_datetime(temp_df['time'])\n",
    "    temp_df = temp_df.set_index('time')\n",
    "    temp_df.index.name = 'Datetime'\n",
    "    temp_df = temp_df.rename(columns={'temperature_2m': 'current_temp'}) # rename column for temperature at 2 meters\n",
    "    print(\"Temperature data loaded successfully.\")\n",
    "    print(temp_df.head())\n",
    "    print(temp_df.info())\n",
    "else:\n",
    "    print(\"Error fetching temperature data:\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7603d8d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7603d8d8",
    "outputId": "1ff318af-c0cb-447e-a52c-aa218d4b3198"
   },
   "outputs": [],
   "source": [
    "# Merge temp_df with the main energy consumption DataFrame (df)\n",
    "df = df.merge(temp_df[['current_temp']], left_index=True, right_index=True, how='left')\n",
    "df.rename(columns={'current_temp': 'temperature'}, inplace=True)\n",
    "\n",
    "print(\"Merged df (first 5 rows):\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c86ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea1c86ac",
    "outputId": "8bfc5b79-e771-478e-f203-c2c9abff533a"
   },
   "outputs": [],
   "source": [
    "# Create temperature-derived features\n",
    "\n",
    "# Lagged temperature feature\n",
    "df['lag_24_temp'] = df['temperature'].shift(24)\n",
    "\n",
    "# 3-day rolling average temperature\n",
    "df['rolling_72_temp_avg'] = df['temperature'].rolling(window=72).mean() # 3-day rolling average\n",
    "\n",
    "# Squared temperature terms\n",
    "df['temp_squared'] = np.power(df['temperature'], 2)\n",
    "\n",
    "# There are NaN values at the beginning of these shifted features, which are expected since there is no other data\n",
    "# to shift from the past. They will need to be handled before modelling by dropping or imputing them carefully.\n",
    "print(\"\\nDataFrame after adding temperature-derived features (first few rows - will have NaNs):\")\n",
    "print(df.head(200))\n",
    "print(\"\\nNumber of NaNs in temperature_derived features:\")\n",
    "print(df[['lag_24_temp', 'rolling_72_temp_avg', 'temp_squared']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac11105",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bac11105",
    "outputId": "60042b43-75fe-4e0d-b2fd-b50f53e3d61d"
   },
   "outputs": [],
   "source": [
    "# Drop the NaN values\n",
    "# Given that the dataset covers years of hourly data from 2002 to 2018, which is over 140,000 hours, the maximum number\n",
    "# of NaN values is very small in relative to the entire dataset size (0.12%), so it is acceptable to drop the rows with\n",
    "# NaN values for time series data.\n",
    "\n",
    "# Create a list of all feature columns that may contain NaN values\n",
    "feature_columns_to_check = [\n",
    "    'lag_1_hour', 'lag_24_hour', 'lag_168_hour', 'lag_24_temp', 'rolling_72_temp_avg', 'temp_squared'\n",
    "]\n",
    "\n",
    "# Drop rows where any of these specified feature columns have NaN values\n",
    "df.dropna(subset=feature_columns_to_check, inplace=True)\n",
    "\n",
    "# Verify that the rows with NaN values are gone\n",
    "print(\"Number of NaNs after dropping:\")\n",
    "print(df[feature_columns_to_check].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd461da9-722b-4b02-9566-f7b73681337f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd461da9-722b-4b02-9566-f7b73681337f",
    "outputId": "7b0df66b-28eb-4fda-ca14-1fb864c750d1"
   },
   "outputs": [],
   "source": [
    "print(\"Columns in df BEFORE data splitting:\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1976b0e-0047-4cb9-91ea-43a4b214fd64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f1976b0e-0047-4cb9-91ea-43a4b214fd64",
    "outputId": "e0f54f38-d216-406c-e1bb-83ba16ea7386"
   },
   "outputs": [],
   "source": [
    "# Visualise selective key features\n",
    "\n",
    "# Set a consistent plot style for better aesthetics\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "\n",
    "# PJME_MW over time\n",
    "# Overall trends of energy consumption, focusing on a recent 2-year period for clarity\n",
    "# As plotting the entire dataset could be overwhelming, a subset is used the initial exploration.\n",
    "df['PJME_MW'].loc['2016':'2017'].plot(\n",
    "    title='PJME_MW Consumption in 2016 and 2017',\n",
    "    ylabel='Energy (MW)',\n",
    "    color='chocolate'\n",
    ")\n",
    "plt.savefig('images/ph2_pjme_mw_over_time.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# PJME_MW vs Hour of Day (by Weekday/Weekend)\n",
    "# The impact of energy consumption based on hours of the day and differentiated by weekdays and weekends\n",
    "df['is_weekend_label'] = df['is_weekend'].map({0: 'No', 1: 'Yes'})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x='hour_of_day',\n",
    "    y='PJME_MW',\n",
    "    hue='is_weekend_label', # differentiate between weekdays (0) and weekends (1)\n",
    "    ax=ax,\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "ax.set_title('PJME_MW Consumption by Hour of Day (Weekday vs Weekend)')\n",
    "ax.set_xlabel('Hour of Day')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "plt.legend(title='Is Weekend?')\n",
    "plt.savefig('images/ph2_pjme_mw_vs_hour_of_day.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# PJME_MW vs Temperature\n",
    "# Direct correlation between energy demand and temperature of day\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='temperature',\n",
    "    y='PJME_MW',\n",
    "    alpha=0.2, # handle overplotting for large datasets\n",
    "    ax=ax,\n",
    "    color='teal'\n",
    ")\n",
    "\n",
    "ax.set_title('PJME_MW vs Temperature')\n",
    "ax.set_xlabel('Temperature (°C)')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "plt.savefig('images/ph2_pjme_mw_vs_temperature.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# Average PJME_MW by Hour and Day of Week\n",
    "# Typical demand for every hour of every day of the week\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "\n",
    "pivot_table = df.pivot_table(values='PJME_MW', index='hour_of_day', columns='day_of_week')\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "sns.heatmap(pivot_table,\n",
    "            cmap='magma',\n",
    "            annot=True, # display the annotation numbers\n",
    "            fmt=\".0f\", # format the numbers as integers\n",
    "            linewidths=0.5,\n",
    "            annot_kws={\"fontsize\": 10},\n",
    "            ax=ax,\n",
    "            cbar_kws={'label': 'Energy (MW)'}\n",
    "           )\n",
    "\n",
    "plt.title('Average PJME_MW by Hour of Day and Day of Week', fontsize=plt.rcParams['axes.titlesize'])\n",
    "plt.xlabel('Day of Week', fontsize=plt.rcParams['axes.labelsize'])\n",
    "plt.ylabel('Hour of Day', fontsize=plt.rcParams['axes.labelsize'])\n",
    "\n",
    "ax.set_xticklabels(day_names)\n",
    "ax.tick_params(axis='x', labelsize=10)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "cbar_ax = fig.axes[-1]\n",
    "cbar_ax.tick_params(labelsize=8)\n",
    "plt.savefig('images/ph2_average_pjme_mw_by_hour_and_day_of_week.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c63c47-676b-46f4-b749-97a65a90d29a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86c63c47-676b-46f4-b749-97a65a90d29a",
    "outputId": "e03e7c26-eb9b-4989-d641-4bf0dd588e3b"
   },
   "outputs": [],
   "source": [
    "# Phase 3: Model Selection and Training\n",
    "\n",
    "# 1. Import Additional Libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from lightgbm.callback import early_stopping\n",
    "\n",
    "print(\"\\nAdditional model selection and training libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ec27f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "500ec27f",
    "outputId": "d86a6fb0-ac16-4d02-c76d-b85d8f220c65"
   },
   "outputs": [],
   "source": [
    "# 2. Data Splitting and Feature/Target Definition\n",
    "# A chronological split with a cutoff date is the approach adopted to prepare the dataset for model training and\n",
    "# evaluation. This method is critical for time series data to prevent data leakage, which would otherwise occur with\n",
    "# random splitting. Data leakage exposes the model to \"future information\" during training, leading to overly optimistic\n",
    "# performance estimates on the test set.\n",
    "# For this project, our choice of cutoff date for the test set is 01 January 2017, which roughly encompasses the last\n",
    "# 1.5 years of the dataset.\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = '2017-01-01'\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "# The target variable is 'PJME_MW'\n",
    "target = 'PJME_MW'\n",
    "\n",
    "# List out all final features excluding the target variable\n",
    "# Ensure that only the intended features are used and makes the model's inputs very clear for everyone\n",
    "# Check the list of feature columns: features = [col for col in df.columns if col != target]\n",
    "\n",
    "features = [\n",
    "    'hour_of_day', 'day_of_week', 'day_of_year', 'week_of_year', 'month', 'quarter', 'year', 'is_weekend',\n",
    "    'day_of_month', 'hour_sin', 'hour_cos', 'dayofyear_sin', 'dayofyear_cos', 'lag_1_hour', 'lag_24_hour', 'lag_168_hour',\n",
    "    'is_holiday', 'temperature', 'lag_24_temp', 'rolling_72_temp_avg', 'temp_squared']\n",
    "\n",
    "# Split the data chronologically using the explicit feature list\n",
    "X_train = df[df.index < cutoff_date][features]\n",
    "y_train = df[df.index < cutoff_date][target]\n",
    "\n",
    "X_test = df[df.index >= cutoff_date][features]\n",
    "y_test = df[df.index >= cutoff_date][target]\n",
    "\n",
    "print(f\"Full data shape: {df.shape}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(f\"\\nTrain data range: {X_train.index.min()} to {X_train.index.max()}\")\n",
    "print(f\"Test data range: {X_test.index.min()} to {X_test.index.max()}\")\n",
    "\n",
    "# Verify that there is no overlap\n",
    "if X_train.index.max() < X_test.index.min():\n",
    "    print(\"\\nData split verified: No temporal overlap between train and test sets.\")\n",
    "else:\n",
    "    print(\"\\nWarning: Temporal overlap detected. Review your cutoff date or splitting logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bc241-10bd-467a-8900-26b534ae6d61",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "250bc241-10bd-467a-8900-26b534ae6d61",
    "outputId": "8a69e240-8f68-4dfc-fbf7-c72cff83ee92"
   },
   "outputs": [],
   "source": [
    "print(\"\\nColumns in X_test AFTER data splitting:\")\n",
    "print(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62929c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "6d62929c",
    "outputId": "a37e4340-9f61-4975-cf53-770df9196eac"
   },
   "outputs": [],
   "source": [
    "# 3. Initial Model Training (for Baseline and Feature Importance)\n",
    "# Powerful gradient boosting models such as XGBoost and LightGBM are excellent model choices to start with for time\n",
    "# series forecasting as they can handle complex relationships well and provide feature importance scores.\n",
    "\n",
    "# --- XGBoost Model ---\n",
    "\n",
    "# Initialise the model\n",
    "# For initial training, we can use default or slightly adjusted parameters.\n",
    "# 'n_estimators' is the number of boosting rounds (trees)\n",
    "# 'early_stopping_rounds' can prevent overfitting during training\n",
    "baseline_xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror', # objective for regression tasks\n",
    "    n_estimators=1000, # number of boosting rounds (trees)\n",
    "    learning_rate=0.05, # step size shrinkage to prevent overfitting\n",
    "    random_state=42, # for reproducibility\n",
    "    n_jobs=-1, # to use all available CPU cores\n",
    "    eval_metric='rmse', # evaluate using Root Mean Squared Error\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# Use early_stopping_rounds to prevent overfitting and eval_set to monitor performance on a validation set\n",
    "# We will use X_test as a simple validation here but for true tuning, it would be more ideal to use a separate validation\n",
    "# split from the training data or a time series cross-validation strategy.\n",
    "# For baseline, using X_test as eval_set is acceptable to see initial performance.\n",
    "baseline_xgb_model.fit(X_train, y_train,\n",
    "                       eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                       verbose=False # surpress all printing during training\n",
    "             )\n",
    "\n",
    "# Make predictions on the test set\n",
    "baseline_xgb_y_pred = baseline_xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the baseline model's performance\n",
    "baseline_xgb_rmse = np.sqrt(mean_squared_error(y_test, baseline_xgb_y_pred))\n",
    "baseline_xgb_mae = mean_absolute_error(y_test, baseline_xgb_y_pred)\n",
    "baseline_xgb_r2 = r2_score(y_test, baseline_xgb_y_pred)\n",
    "\n",
    "print(f\"XGBoost Baseline Model Performance:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {baseline_xgb_rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {baseline_xgb_mae:.2f}\")\n",
    "print(f\"R-squared (R^2): {baseline_xgb_r2:.5f}\")\n",
    "\n",
    "# Visualise Predictions vs Actuals\n",
    "# Create a DataFrame for easy plotting\n",
    "baseline_xgb_results = pd.DataFrame({'Actual': y_test, 'Predicted': baseline_xgb_y_pred}, index=y_test.index)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(baseline_xgb_results.index, baseline_xgb_results['Actual'], label='Actual Consumption', alpha=0.7)\n",
    "plt.plot(baseline_xgb_results.index, baseline_xgb_results['Predicted'], label='Predicted Consumption', alpha=0.8)\n",
    "plt.title('XGBoost Baseline Model: Actual vs Predicted Energy Consumption')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('PJME_MW')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('images/ph3_xgboost_baseline_model.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895fd7ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "id": "895fd7ad",
    "outputId": "a28c2acf-b206-4212-85ba-47e336a33f2d"
   },
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "baseline_xgb_feature_importances = pd.DataFrame(\n",
    "    {'feature': X_train.columns, 'importance': baseline_xgb_model.feature_importances_}\n",
    ").sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Feature Importances from Baseline XGBoost Model:\")\n",
    "print(baseline_xgb_feature_importances.head(10))\n",
    "\n",
    "# Visualise all feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=baseline_xgb_feature_importances.head(20)) # top 20 features\n",
    "plt.title('XGBoost Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph3_top_10_features_xgboost_baseline_model.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2a25e-73f7-4263-a38b-fb988d0fa4c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "4aa2a25e-73f7-4263-a38b-fb988d0fa4c1",
    "outputId": "3369c726-dd7d-4765-c323-820301be4519"
   },
   "outputs": [],
   "source": [
    "# --- LightGBM Model ---\n",
    "\n",
    "# Initialise the model\n",
    "# 'n_estimators' is the number of boosting rounds (trees)\n",
    "baseline_lgbm_model = lgb.LGBMRegressor(\n",
    "    objective='regression', # objective for regression tasks\n",
    "    n_estimators=1000, # number of boosting rounds (trees)\n",
    "    learning_rate=0.05, # step size shrinkage to prevent overfitting\n",
    "    random_state=42, # for reproducibility\n",
    "    n_jobs=-1 # to use all available CPU cores\n",
    ")\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping_callback_lgbm = lgb.early_stopping(\n",
    "    stopping_rounds=50, # number of rounds with no improvement to stop\n",
    "    verbose=False # suppress printing during training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# Use eval_set and callbacks to monitor performance on a validation set\n",
    "baseline_lgbm_model.fit(X_train, y_train,\n",
    "              eval_set=[(X_test, y_test)], # using X_test for validation\n",
    "              eval_metric='rmse' # evaluate using Root Mean Squared Error\n",
    "              )\n",
    "\n",
    "# Make predictions on the test set\n",
    "baseline_lgbm_y_pred = baseline_lgbm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the baseline model's performance\n",
    "baseline_lgbm_rmse = np.sqrt(mean_squared_error(y_test, baseline_lgbm_y_pred))\n",
    "baseline_lgbm_mae = mean_absolute_error(y_test, baseline_lgbm_y_pred)\n",
    "baseline_lgbm_r2 = r2_score(y_test, baseline_lgbm_y_pred)\n",
    "\n",
    "print(f\"\\nLightGBM Baseline Model Performance:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {baseline_lgbm_rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {baseline_lgbm_mae:.2f}\")\n",
    "print(f\"R-squared (R^2): {baseline_lgbm_r2:.5f}\")\n",
    "\n",
    "# Visualise Predictions vs Actuals\n",
    "# Create a DataFrame for easy plotting\n",
    "baseline_lgbm_results = pd.DataFrame({'Actual': y_test, 'Predicted': baseline_lgbm_y_pred}, index=y_test.index)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(baseline_lgbm_results.index, baseline_lgbm_results['Actual'], label='Actual Consumption', alpha=0.7)\n",
    "plt.plot(baseline_lgbm_results.index, baseline_lgbm_results['Predicted'], label='Predicted Consumption', alpha=0.8)\n",
    "plt.title('LightGBM Baseline Model: Actual vs Predicted Energy Consumption')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('PJME_MW')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('images/ph3_lightgbm_baseline_model.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848767f0-ea83-42b4-9b10-f48e4fec5485",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "id": "848767f0-ea83-42b4-9b10-f48e4fec5485",
    "outputId": "12ebb552-1051-4335-f432-3afdd90a6b05"
   },
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "baseline_lgbm_feature_importances = pd.DataFrame(\n",
    "    {'feature': X_train.columns, 'importance': baseline_lgbm_model.feature_importances_}\n",
    ").sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Feature Importances from Baseline LightGBM Model:\")\n",
    "print(baseline_lgbm_feature_importances.head(10))\n",
    "\n",
    "# Visualise all feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=baseline_lgbm_feature_importances.head(20)) # top 20 features\n",
    "plt.title('LightGBM Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph3_top_10_features_lightgbm_baseline_model.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5544c4d-50c8-4132-b3cb-e5ef35a7f27c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5544c4d-50c8-4132-b3cb-e5ef35a7f27c",
    "outputId": "bb52a15a-06fd-49db-f0b9-eea52e6f580a"
   },
   "outputs": [],
   "source": [
    "# Phase 4: Advanced Feature Engineering\n",
    "\n",
    "# 1.1 Derived Weather Features (Heating and Cooling Degree Days)\n",
    "# Capture more complex, non-linear and domain-specific relationships between weather conditions and\n",
    "# energy consumption as compared to using raw temperature data alone.\n",
    "# The base temperature used is 18°C.\n",
    "\n",
    "# Check if 'Datetime' column exists. Set it if it is not the index already\n",
    "if 'Datetime' in df.columns:\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df = df.set_index('Datetime')\n",
    "# Convert it if 'Datetime' is already the index but its dtype isn't datetime64[ns]\n",
    "elif not isinstance(df.index, pd.DatetimeIndex):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Define a base temperature\n",
    "# 18 degrees Celsius is common for energy forecasting\n",
    "base_temperature_celsius = 18\n",
    "\n",
    "# Calculate daily average temperature\n",
    "# Resample will work correctly now that we're sure df.index is a DatetimeIndex\n",
    "daily_temp_avg = df['temperature'].resample('D').mean()\n",
    "\n",
    "# Calculate HDD and CDD\n",
    "df_daily_hdd_cdd = pd.DataFrame(index=daily_temp_avg.index)\n",
    "df_daily_hdd_cdd['daily_avg_temp'] = daily_temp_avg\n",
    "\n",
    "df_daily_hdd_cdd['hdd'] = (base_temperature_celsius - df_daily_hdd_cdd['daily_avg_temp']).apply(lambda x: max(0, x))\n",
    "df_daily_hdd_cdd['cdd'] = (df_daily_hdd_cdd['daily_avg_temp'] - base_temperature_celsius).apply(lambda x: max(0, x))\n",
    "\n",
    "# Map HDD and CDD back to your original hourly DataFrame\n",
    "# Use .normalize() on the hourly index to match the daily index for mapping\n",
    "df['hdd'] = df.index.normalize().map(df_daily_hdd_cdd['hdd'])\n",
    "df['cdd'] = df.index.normalize().map(df_daily_hdd_cdd['cdd'])\n",
    "\n",
    "# Handle potential NaNs such as if there were gaps or mismatches\n",
    "# Use explicit assignment to avoid the FutureWarning\n",
    "df['hdd'] = df['hdd'].fillna(0)\n",
    "df['cdd'] = df['cdd'].fillna(0)\n",
    "\n",
    "# Inspect the new features\n",
    "print(df[['temperature', 'hdd', 'cdd']].head())\n",
    "print(df[['temperature', 'hdd', 'cdd']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5571416-2ffa-4650-9e08-15f6d2c7cf34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5571416-2ffa-4650-9e08-15f6d2c7cf34",
    "outputId": "d31a98b8-c6db-4901-9323-ce7b160cc812"
   },
   "outputs": [],
   "source": [
    "# 1.2 Rolling Window Statistics\n",
    "# Capture recent trends and volatility of the target variable, `PJME_MW` over specific time windows to\n",
    "# give the model valuable context about the immediate past\n",
    "\n",
    "# 24-hour (1-day) Rolling Mean of PJME_MW\n",
    "# .rolling(window='24H') creates a rolling window of 24 hours.\n",
    "# .mean() calculates the mean within that window.\n",
    "# .bfill() is used here to fill NaN values at the beginning of the series, where there aren't enough\n",
    "# previous data points to fill the window.\n",
    "df['PJME_MW_rolling_24_hr_mean'] = df['PJME_MW'].rolling(window='24h', closed='left').mean().bfill()\n",
    "\n",
    "# 168-hour (7-day) Rolling Mean of PJME_MW\n",
    "df['PJME_MW_rolling_168_hr_mean'] = df['PJME_MW'].rolling(window='168h', closed='left').mean().bfill()\n",
    "\n",
    "# 24-hour (1-day) Rolling Standard Deviation of PJME_MW\n",
    "df['PJME_MW_rolling_24_hr_std'] = df['PJME_MW'].rolling(window='24h', closed='left').std().bfill()\n",
    "\n",
    "# 168-hour (7-day) Rolling Standard Deviation of PJME_MW\n",
    "df['PJME_MW_rolling_168_hr_std'] = df['PJME_MW'].rolling(window='168h', closed='left').std().bfill()\n",
    "\n",
    "# Inspect the new features\n",
    "print(\"\\n--- After adding Rolling Mean Features ---\")\n",
    "print(df[['PJME_MW', 'PJME_MW_rolling_24_hr_mean', 'PJME_MW_rolling_168_hr_mean']].head(40)) # show more rows to see rolling effect\n",
    "print(df[['PJME_MW', 'PJME_MW_rolling_24_hr_mean', 'PJME_MW_rolling_168_hr_mean']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27901b2c-a047-4562-97e9-459640fa4a23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27901b2c-a047-4562-97e9-459640fa4a23",
    "outputId": "735b2c10-00ae-41cd-97b1-74bf2949d5fa"
   },
   "outputs": [],
   "source": [
    "# 1.3 Interaction Features\n",
    "# Created by combining two or more existing features to help the model capture relationships that are\n",
    "# less apparent from individual features alone.\n",
    "\n",
    "# Hour of day interacted with `is_weekend`\n",
    "# Creates a unique value for each hour on a weekend vs on a weekday\n",
    "# 1 is added to is_weekend (0 or 1) to distinguish weekend hours.\n",
    "df['hour_of_day_x_is_weekend'] = df['hour_of_day'] * (df['is_weekend'] + 1)\n",
    "\n",
    "# Temperature interacted with `is_holiday`\n",
    "# Highlights temperature effects specifically on holidays\n",
    "# 1 is added to is_holiday (0 or 1) to distinguish holiday temperatures.\n",
    "df['temperature_x_is_holiday'] = df['temperature'] * (df['is_holiday'] + 1)\n",
    "\n",
    "# Temperature interacted with `hour_of_day`\n",
    "# Accounts for the varying effect of temperature on demand at different hours of the day\n",
    "df['temperature_x_hour_of_day'] = df['temperature'] * (df['hour_of_day'] + 1)\n",
    "\n",
    "# CDD interacted with `is_weekend`\n",
    "# Captures the potentially different impact of cooling degree days (CDD) on weekends\n",
    "# as compared to weekdays\n",
    "df['cdd_x_is_weekend'] = df['cdd'] * df['is_weekend']\n",
    "\n",
    "# Inspect the new features\n",
    "print(\"\\n--- After adding Interaction Features ---\")\n",
    "print(df[['hour_of_day', 'is_weekend', 'hour_of_day_x_is_weekend',\n",
    "          'temperature', 'is_holiday', 'temperature_x_is_holiday']].head())\n",
    "print(df[['hour_of_day', 'is_weekend', 'hour_of_day_x_is_weekend',\n",
    "          'temperature', 'is_holiday', 'temperature_x_is_holiday']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d86e4-a32b-442c-8c0f-576ab697017b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4d2d86e4-a32b-442c-8c0f-576ab697017b",
    "outputId": "3e9eebe5-de8c-40db-e7f0-1a9fa6c86bda"
   },
   "outputs": [],
   "source": [
    "# Visualising selected advanced features\n",
    "\n",
    "# Global plot aesthetics\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 8\n",
    "plt.rcParams['figure.figsize'] = (15, 7) # Default figure size for horizontal plots\n",
    "\n",
    "# HDD/CDD Visual: PJME_MW vs Heating Degree Days (HDD)\n",
    "# Demonstrate the positive linear relationship between HDD and energy demand as heating needs increase\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='hdd',\n",
    "    y='PJME_MW',\n",
    "    alpha=0.3,\n",
    "    color='firebrick',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('PJME_MW vs Heating Degree Days (HDD)')\n",
    "ax.set_xlabel('HDD')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph4_pjme_mw_vs_hdd.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# HDD/CDD Visual: PJME_MW vs Cooling Degree Days (CDD)\n",
    "# Show the positive linear relationship between CDD and energy demand as cooling needs increase\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='cdd',\n",
    "    y='PJME_MW',\n",
    "    alpha=0.3,\n",
    "    color='darkviolet',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('PJME_MW vs Cooling Degree Days (CDD)')\n",
    "ax.set_xlabel('CDD')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph4_pjme_mw_vs_cdd.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# Rolling Window Visual: PJME_MW vs its 24-Hour Rolling Mean\n",
    "# Visualise the smoothing effect and how the rolling mean captures underlying trends\n",
    "# A subset of data between June and August 2017 is plotted for clarity.\n",
    "start_date_rolling = '2017-06-01'\n",
    "end_date_rolling = '2017-08-31'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "df.loc[start_date_rolling:end_date_rolling, 'PJME_MW'].plot(ax=ax,\n",
    "                                            label='Actual PJME_MW',\n",
    "                                            color='grey',\n",
    "                                            alpha=0.6)\n",
    "df.loc[start_date_rolling:end_date_rolling, 'PJME_MW_rolling_24_hr_mean'].plot(ax=ax,\n",
    "                                                               label='24-Hour Rolling Mean',\n",
    "                                                               color='maroon',\n",
    "                                                               linestyle='--')\n",
    "ax.set_title(f'PJME_MW vs 24-Hour Rolling Mean ({start_date_rolling} to {end_date_rolling})', fontsize=20)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph4_pjme_mw_vs_24_hr_rolling_mean.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# Rolling Window Visual: PJME_MW 24-Hour Rolling Standard Deviation\n",
    "# Show periods of higher or lower volatility in energy demand as captured by the rolling standard deviation\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "df.loc[start_date_rolling:end_date_rolling, 'PJME_MW'].plot(ax=ax,\n",
    "                                                            label='Actual PJME_MW',\n",
    "                                                            color='saddlebrown',\n",
    "                                                            alpha=0.6)\n",
    "df.loc[start_date_rolling:end_date_rolling, 'PJME_MW_rolling_24_hr_mean'].plot(ax=ax,\n",
    "                                                                               label='24-Hour Rolling Mean',\n",
    "                                                                               color='navy',\n",
    "                                                                               linestyle='--')\n",
    "ax.set_title(f'PJME_MW vs 24-Hour Rolling Standard Deviation ({start_date_rolling} to {end_date_rolling})', fontsize=20)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Standard Deviation (MW)')\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph4_pjme_mw_vs_24_hr_rolling_std_dev.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# Interaction Feature: PJME_MW vs Is_Holiday\n",
    "# Show the difference in energy consumption on holidays vs non-holidays\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x='is_holiday',\n",
    "    y='PJME_MW',\n",
    "    hue='is_holiday',\n",
    "    ax=ax,\n",
    "    palette=['goldenrod', 'indigo'],\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "ax.set_title('PJME_MW by Holiday Status')\n",
    "ax.set_xlabel('Is Holiday (0 = No, 1 = Yes)')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph4_pjme_mw_vs_is_holiday.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook\n",
    "\n",
    "# Interaction Feature: PJME_MW vs CDD, colored by Is_Weekend\n",
    "# Depict if the impact of cooling degree days on demand differs on weekends vs weekdays,\n",
    "# highlighting the 'cdd_x_is_weekend' interaction\n",
    "df['is_weekend_label'] = df['is_weekend'].map({0: 'No', 1: 'Yes'})\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='cdd',\n",
    "    y='PJME_MW',\n",
    "    hue='is_weekend_label', # color points based on whether it's a weekend\n",
    "    alpha=0.3,\n",
    "    palette=['crimson', 'forestgreen'], # different palette for clarity\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('PJME_MW vs Cooling Degree Days (CDD) by Weekend Status')\n",
    "ax.set_xlabel('CDD')\n",
    "ax.set_ylabel('Energy (MW)')\n",
    "plt.legend(title='Is Weekend?')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ph4_pjme_mw_vs_cdd_by_is_weekend.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4729e6-3cc4-46b4-b2bc-e7254dca1e6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d4729e6-3cc4-46b4-b2bc-e7254dca1e6b",
    "outputId": "40f62ecc-fc92-43e8-d1ad-c58253f4d2c6"
   },
   "outputs": [],
   "source": [
    "# Phase 5: Hyperparameter Optimisation using Optuna\n",
    "\n",
    "# 1. Import Additional Libraries\n",
    "# Install Optuna using CLI if it is not done already\n",
    "# !pip install xgboost # for Colab\n",
    "# !pip install lightgbm # for Colab\n",
    "!pip install optuna\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import optuna\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "print(\"\\nAdditional hyperparameter optimisation libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gSlWJQykPt1L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSlWJQykPt1L",
    "outputId": "11b3bf49-0f80-45d7-bf0b-59d76c58bbfe"
   },
   "outputs": [],
   "source": [
    "# 2. Data Splitting and Feature/Target Definition\n",
    "# A similar data splitting approach to Phase 3 (Model Selection and Training) is adopted to prepare the refined\n",
    "# dataset for model tuning and evaluation.\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = '2017-01-01'\n",
    "\n",
    "# Create the training and test sets\n",
    "# .copy is used to avoid SettingWithCopyWarning\n",
    "df_train = df.loc[df.index < cutoff_date].copy()\n",
    "df_test = df.loc[df.index >= cutoff_date].copy()\n",
    "\n",
    "# Define the features (X) and target (y)\n",
    "# The target variable is 'PJME_MW'\n",
    "target = 'PJME_MW'\n",
    "exclude_features = ['is_weekend_label']\n",
    "\n",
    "# List out all raw and newly engineering features excluding the target variable\n",
    "# Check the list of feature columns: features = [col for col in df.columns if col != target and col not in exclude_features]\n",
    "\n",
    "features = ['hour_of_day', 'day_of_week', 'day_of_year', 'week_of_year', 'month', 'quarter', 'year', 'is_weekend',\n",
    "            'day_of_month', 'hour_sin', 'hour_cos', 'dayofyear_sin', 'dayofyear_cos', 'lag_1_hour', 'lag_24_hour',\n",
    "            'is_holiday', 'temperature', 'lag_24_temp', 'rolling_72_temp_avg', 'temp_squared', 'hdd', 'cdd',\n",
    "            'PJME_MW_rolling_24_hr_mean', 'PJME_MW_rolling_168_hr_mean',\n",
    "            'PJME_MW_rolling_24_hr_std', 'PJME_MW_rolling_168_hr_std', 'hour_of_day_x_is_weekend',\n",
    "            'temperature_x_is_holiday', 'temperature_x_hour_of_day', 'cdd_x_is_weekend']\n",
    "\n",
    "# Create the final feature and target DataFrames for both training and testing\n",
    "X_train = df_train[features]\n",
    "y_train = df_train[target]\n",
    "X_test = df_test[features]\n",
    "y_test = df_test[target]\n",
    "\n",
    "print(f\"Training set has {X_train.shape[0]} rows and {X_train.shape[1]} columns.\")\n",
    "print(f\"Test set has {X_test.shape[0]} rows and {X_test.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x5aOztEWO59B",
   "metadata": {
    "id": "x5aOztEWO59B"
   },
   "outputs": [],
   "source": [
    "# 3. Model Tuning\n",
    "\n",
    "# --- XGBoost Model ---\n",
    "\n",
    "# Define the objective function for Optuna to optimise hyperparameters\n",
    "def xgb_objective(trial):\n",
    "    \"\"\"\n",
    "    Define the objective function for Optuna to optimise hyperparameters for the XGBoost model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for NaN or infinite values in the data before starting the loop\n",
    "    if X_train.isnull().values.any() or not np.isfinite(X_train).all().all():\n",
    "        print(\"Error: X_train contains NaN or infinite values. Please clean your data.\")\n",
    "        return float('inf')\n",
    "    if y_train.isnull().values.any() or not np.isfinite(y_train).all():\n",
    "        print(\"Error: y_train contains NaN or infinite values. Please clean your data.\")\n",
    "        return float('inf')\n",
    "\n",
    "    # Suggest hyperparameters to be optimised\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'rmse',\n",
    "        'early_stopping_rounds': 50,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    # Train the model with suggested parameters\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    oof_predictions = np.zeros(len(X_train))\n",
    "\n",
    "    # Track the number of folds that were successfully processed\n",
    "    valid_folds = 0\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
    "      try:\n",
    "        print(f\"Trial {trial.number}, Fold {fold}: train_size={len(train_index)}, val_size={len(val_index)}\")\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                  eval_set=[(X_val_fold, y_val_fold)],\n",
    "                  verbose=False\n",
    "                 )\n",
    "\n",
    "        # Make predictions\n",
    "        oof_predictions[val_index] = model.predict(X_val_fold)\n",
    "        valid_folds =+ 1\n",
    "\n",
    "        # Delete variables to manage memory\n",
    "        del X_train_fold, X_val_fold, y_train_fold, y_val_fold\n",
    "\n",
    "      except UnboundLocalError:\n",
    "        # Continue if the error occurs due to the fold being invalid\n",
    "        print(f\"Trial {trial.number}, Fold {fold}: Skipping due to an invalid split.\")\n",
    "        continue\n",
    "\n",
    "    # Return a high value to indicate a failed trial if no folds were created\n",
    "    if valid_folds == 0:\n",
    "      return float('inf')\n",
    "\n",
    "    # Calculate the evaluation metric (RMSE)\n",
    "    rmse = np.sqrt(mean_squared_error(y_train, oof_predictions))\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdddf9-4749-49f0-9dfd-ec79cf720b5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0e375bfd205143bbbbb73cd302d5da91",
      "b80f7d9df2d14fcb9be52193bd2b434d",
      "e912f0c09b2d46eb9158012b9f5bcc66",
      "45f3a202ab7e46fa9f197714f5e55f16",
      "f10ecbb062d744fd99e44d65f5f79829",
      "6d9dd9aa74ca4e4d90f79b8de63689a4",
      "66e45f80b8fc4e9082578c71b8218edf",
      "ee8e7bfd4c6e4ea6989ce9310760845b",
      "ab06fcac969542aab442b8446e573202",
      "8852098bc03a4d6dbaad343c159123e9",
      "4f89567e8c344dd894067d1f36d0fb4f"
     ]
    },
    "id": "78fdddf9-4749-49f0-9dfd-ec79cf720b5a",
    "outputId": "dd99bafb-9c61-4a40-d650-93560210a820"
   },
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimise the objective function\n",
    "\n",
    "print(\"\\nStarting hyperparameter optimisation with Optuna for XGBoost model...\")\n",
    "xgb_study = optuna.create_study(direction='minimize')\n",
    "xgb_study.optimize(xgb_objective, n_trials=50, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fc99e-6eb2-48da-a969-4162aec0c650",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "180fc99e-6eb2-48da-a969-4162aec0c650",
    "outputId": "bc6c09d3-ed89-44d7-b101-8959607aba84"
   },
   "outputs": [],
   "source": [
    "# View optimisation results\n",
    "\n",
    "print(\"\\n-- XGBoost Hyperparameter Optimisation Results --\")\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value (RMSE): {xgb_study.best_trial.value:.2f}\")\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for key, value in xgb_study.best_trial.params.items():\n",
    "    print(f\"  {key}:{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbac6909-2e53-43f4-a057-2b57df6fc12c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "dbac6909-2e53-43f4-a057-2b57df6fc12c",
    "outputId": "0ede3e6c-a143-4c70-a669-c7739d6e191c"
   },
   "outputs": [],
   "source": [
    "# Train the tuned model with best hyperparameters and evaluate\n",
    "# Get the best hyperparameters from Optuna\n",
    "best_xgb_params = xgb_study.best_trial.params\n",
    "\n",
    "# Initialise and train the tuned XGBoost model with the best hyperparameters\n",
    "# Train the model on the full training set, and use the test set for evaluation\n",
    "tuned_xgb_model = xgb.XGBRegressor(**best_xgb_params)\n",
    "tuned_xgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "# Make predictions with the tuned model\n",
    "tuned_xgb_y_pred = tuned_xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the tuned model's performance\n",
    "tuned_xgb_rmse = np.sqrt(mean_squared_error(y_test, tuned_xgb_y_pred))\n",
    "tuned_xgb_mae = mean_absolute_error(y_test, tuned_xgb_y_pred)\n",
    "tuned_xgb_r2 = r2_score(y_test, tuned_xgb_y_pred)\n",
    "\n",
    "print(f\"XGBoost Tuned Model Performance:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {tuned_xgb_rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {tuned_xgb_mae:.2f}\")\n",
    "print(f\"R-squared (R^2): {tuned_xgb_r2:.5f}\")\n",
    "\n",
    "# Visualise Predictions vs Actuals\n",
    "# Create a DataFrame for easy plotting\n",
    "tuned_xgb_results = pd.DataFrame({'Actual': y_test, 'Predicted': tuned_xgb_y_pred}, index=y_test.index)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(tuned_xgb_results.index, tuned_xgb_results['Actual'], label='Actual Consumption', alpha=0.7)\n",
    "plt.plot(tuned_xgb_results.index, tuned_xgb_results['Predicted'], label='Predicted Consumption', alpha=0.8)\n",
    "plt.title('XGBoost Tuned Model: Actual vs Predicted Energy Consumption')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('PJME_MW')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('images/ph5_xgboost_tuned_model.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b688d5-07ee-44da-a578-19b08337ffd2",
   "metadata": {
    "id": "c0b688d5-07ee-44da-a578-19b08337ffd2"
   },
   "outputs": [],
   "source": [
    "# --- LightGBM Model ---\n",
    "\n",
    "# Define the objective function for Optuna to optimise hyperparameters\n",
    "def lgbm_objective(trial):\n",
    "    \"\"\"\n",
    "    Define the objective function for Optuna to optimise hyperparameters for the LightGBM model.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters to be optimised\n",
    "    params = {\n",
    "        'objective': 'regression_l1',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'metric': 'rmse',\n",
    "        'early_stopping_rounds': 50,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    # Train the model with suggested parameters\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    oof_predictions = np.zeros(len(X_train))\n",
    "\n",
    "    # Track the number of folds that were successfully processed\n",
    "    valid_folds = 0\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(tscv.split(X_train)):\n",
    "      try:\n",
    "        print(f\"Trial {trial.number}, Fold {fold}: train_size={len(train_index)}, val_size={len(val_index)}\")\n",
    "\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold,\n",
    "                  eval_set=[(X_val_fold, y_val_fold)],\n",
    "                  callbacks=[early_stopping(stopping_rounds=50, verbose=False)]\n",
    "                 )\n",
    "\n",
    "        # Make predictions\n",
    "        oof_predictions = model.predict(X_test)\n",
    "        fold_count =+ 1\n",
    "\n",
    "        # Delete variables to manage memory\n",
    "        del X_train_fold, X_val_fold, y_train_fold, y_val_fold\n",
    "\n",
    "      except UnboundLocalError:\n",
    "        # Continue if the error occurs due to the fold being invalid\n",
    "        print(f\"Trial {trial.number}, Fold {fold}: Skipping due to an invalid split.\")\n",
    "        continue\n",
    "\n",
    "    # Return a high value to indicate a failed trial if no folds were created\n",
    "    if valid_folds == 0:\n",
    "      return float('inf')\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_train, oof_predictions))\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4528ee5-140e-4e67-a014-1e3a2b31cc94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c216d10d27684e579fc9f4b495024212",
      "6952b4430a524343beac9a8a3dd65ab3",
      "6ed14a30ef644bbbbe5947d5d3e2ed2b",
      "747d6c0e58a94884983518852af255c7",
      "b9d988d43ba84b9593271d7ec904588b",
      "a242f6f677d44e26bb495905a63356cc",
      "2a66a17dc3f04f79aa031c08095b6b96",
      "f86f0a77d97e41aaa0ac3083dddea4fd",
      "4f788250ab114367a7ce2ac5d6a406ee",
      "24b20afc2bfa46e99cf6330a64e76ff0",
      "b5865df6b8ee44abb1c457d686c643b2"
     ]
    },
    "id": "a4528ee5-140e-4e67-a014-1e3a2b31cc94",
    "outputId": "5be25c27-f0e6-4a33-ba04-39b39d60f888"
   },
   "outputs": [],
   "source": [
    "# Create an Optuna study and optimise the objective function\n",
    "\n",
    "print(\"\\nStarting hyperparameter optimisation with Optuna for LightGBM model...\")\n",
    "lgbm_study = optuna.create_study(direction='minimize')\n",
    "lgbm_study.optimize(lgbm_objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n-- LightGBM Hyperparameter Optimisation Results --\")\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value (RMSE): {lgbm_study.best_trial.value:.2f}\")\n",
    "print(\"  Best Hyperparameters:\")\n",
    "for key, value in lgbm_study.best_trial.params.items():\n",
    "    print(f\"  {key}:{value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8d8a8-596e-4087-89cd-1e84f3035aaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "id": "45b8d8a8-596e-4087-89cd-1e84f3035aaf",
    "outputId": "5d94920c-0e29-40d4-de46-215fc4d9867c"
   },
   "outputs": [],
   "source": [
    "# Train the tuned model with best hyperparameters and evaluate\n",
    "# Get the best hyperparameters from Optuna\n",
    "best_lgbm_params = lgbm_study.best_trial.params\n",
    "\n",
    "# Initialise and train the tuned XGBoost model with the best hyperparameters\n",
    "tuned_lgbm_model = xgb.XGBRegressor(**best_lgbm_params)\n",
    "tuned_lgbm_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "# Make predictions with the tuned model\n",
    "tuned_lgbm_y_pred = tuned_lgbm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the tuned model's performance\n",
    "tuned_lgbm_rmse = np.sqrt(mean_squared_error(y_test, tuned_lgbm_y_pred))\n",
    "tuned_lgbm_mae = mean_absolute_error(y_test, tuned_lgbm_y_pred)\n",
    "tuned_lgbm_r2 = r2_score(y_test, tuned_lgbm_y_pred)\n",
    "\n",
    "print(f\"\\nLightGBM Tuned Model Performance:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {tuned_lgbm_rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {tuned_lgbm_mae:.2f}\")\n",
    "print(f\"R-squared (R^2): {tuned_lgbm_r2:.5f}\")\n",
    "\n",
    "# Visualise Predictions vs Actuals\n",
    "# Create a DataFrame for easy plotting\n",
    "tuned_lgbm_results = pd.DataFrame({'Actual': y_test, 'Predicted': tuned_lgbm_y_pred}, index=y_test.index)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(tuned_lgbm_results.index, tuned_lgbm_results['Actual'], label='Actual Consumption', alpha=0.7)\n",
    "plt.plot(tuned_lgbm_results.index, tuned_lgbm_results['Predicted'], label='Predicted Consumption', alpha=0.8)\n",
    "plt.title('LightGBM Tuned Model: Actual vs Predicted Energy Consumption')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('PJME_MW')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('images/ph5_lgbm_tuned_model.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8W5zFhZUGHAp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8W5zFhZUGHAp",
    "outputId": "5673ada5-f048-4673-f749-766dfc51b21e"
   },
   "outputs": [],
   "source": [
    "# Compare baseline and tuned models for both XGBoost and LightGBM\n",
    "\n",
    "# Create a dictionary to store the performance metrics for each model\n",
    "metrics = {\n",
    "    'Baseline_XGBoost': {\n",
    "        'RMSE': baseline_xgb_rmse,\n",
    "        'MAE': baseline_xgb_mae,\n",
    "        'R^2': baseline_xgb_r2\n",
    "    },\n",
    "    'Tuned_XGBoost': {\n",
    "        'RMSE': tuned_xgb_rmse,\n",
    "        'MAE': tuned_xgb_mae,\n",
    "        'R^2': tuned_xgb_r2\n",
    "    },\n",
    "    'Baseline_LightGBM': {\n",
    "        'RMSE': baseline_lgbm_rmse,\n",
    "        'MAE': baseline_lgbm_mae,\n",
    "        'R^2': baseline_lgbm_r2\n",
    "    },\n",
    "    'Tuned_LightGBM': {\n",
    "        'RMSE': tuned_lgbm_rmse,\n",
    "        'MAE': tuned_lgbm_mae,\n",
    "        'R^2': tuned_lgbm_r2\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame from the metrics dictionary for easy comparison\n",
    "metrics_df = pd.DataFrame(metrics).transpose()\n",
    "metrics_df = metrics_df.round(5)\n",
    "\n",
    "print(\"---Model Performance Comparison Table ---\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f72ba4-47f5-4549-8d21-5595d44813ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "f7f72ba4-47f5-4549-8d21-5595d44813ce",
    "outputId": "6647c0b8-aba5-4a61-bc17-28b827964dc3"
   },
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame for easy plotting\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Baseline_XGBoost': baseline_xgb_y_pred,\n",
    "    'Tuned_XGBoost': tuned_xgb_y_pred,\n",
    "    'Baseline_LightGBM': baseline_lgbm_y_pred,\n",
    "    'Tuned_LightGBM': tuned_lgbm_y_pred\n",
    "}, index=y_test.index)\n",
    "\n",
    "plt.figure(figsize=(22, 10))\n",
    "\n",
    "plt.plot(comparison_df.index, comparison_df['Actual'],\n",
    "         label='Actual Consumption', color='black',\n",
    "         alpha=0.9, linewidth=2.5)\n",
    "\n",
    "plt.plot(comparison_df.index, comparison_df['Baseline_XGBoost'],\n",
    "         label='XGBoost Baseline', color='royalblue',\n",
    "         linestyle='--', alpha=0.7)\n",
    "plt.plot(comparison_df.index, comparison_df['Tuned_XGBoost'],\n",
    "         label='XGBoost Tuned', color='royalblue', alpha=0.8, linewidth=1.5)\n",
    "\n",
    "plt.plot(comparison_df.index, comparison_df['Baseline_LightGBM'],\n",
    "         label='LightGBM Baseline', color='darkorange',\n",
    "         linestyle='--', alpha=0.7)\n",
    "plt.plot(comparison_df.index, comparison_df['Tuned_LightGBM'],\n",
    "         label='LightGBM Tuned', color='darkorange', alpha=0.8, linewidth=1.5)\n",
    "\n",
    "plt.title('Comparison of Model Predictions vs Actual Energy Consumption', fontsize=18)\n",
    "plt.xlabel('Datetime', fontsize=14)\n",
    "plt.ylabel('PJME_MW', fontsize=14)\n",
    "plt.legend(loc='upper left', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('images/ph5_baseline_tuned_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show() # uncomment this line to display visual while running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40981684-1245-4ced-a97f-d6c198a4334e",
   "metadata": {
    "id": "40981684-1245-4ced-a97f-d6c198a4334e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ML Projects)",
   "language": "python",
   "name": "ml-projects-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0e375bfd205143bbbbb73cd302d5da91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b80f7d9df2d14fcb9be52193bd2b434d",
       "IPY_MODEL_e912f0c09b2d46eb9158012b9f5bcc66",
       "IPY_MODEL_45f3a202ab7e46fa9f197714f5e55f16"
      ],
      "layout": "IPY_MODEL_f10ecbb062d744fd99e44d65f5f79829"
     }
    },
    "24b20afc2bfa46e99cf6330a64e76ff0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a66a17dc3f04f79aa031c08095b6b96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45f3a202ab7e46fa9f197714f5e55f16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8852098bc03a4d6dbaad343c159123e9",
      "placeholder": "​",
      "style": "IPY_MODEL_4f89567e8c344dd894067d1f36d0fb4f",
      "value": " 50/50 [4:02:05&lt;00:00, 205.80s/it]"
     }
    },
    "4f788250ab114367a7ce2ac5d6a406ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f89567e8c344dd894067d1f36d0fb4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66e45f80b8fc4e9082578c71b8218edf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6952b4430a524343beac9a8a3dd65ab3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a242f6f677d44e26bb495905a63356cc",
      "placeholder": "​",
      "style": "IPY_MODEL_2a66a17dc3f04f79aa031c08095b6b96",
      "value": "Best trial: 0. Best value: inf: 100%"
     }
    },
    "6d9dd9aa74ca4e4d90f79b8de63689a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ed14a30ef644bbbbe5947d5d3e2ed2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f86f0a77d97e41aaa0ac3083dddea4fd",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4f788250ab114367a7ce2ac5d6a406ee",
      "value": 50
     }
    },
    "747d6c0e58a94884983518852af255c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24b20afc2bfa46e99cf6330a64e76ff0",
      "placeholder": "​",
      "style": "IPY_MODEL_b5865df6b8ee44abb1c457d686c643b2",
      "value": " 50/50 [1:42:49&lt;00:00, 131.19s/it]"
     }
    },
    "8852098bc03a4d6dbaad343c159123e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a242f6f677d44e26bb495905a63356cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab06fcac969542aab442b8446e573202": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b5865df6b8ee44abb1c457d686c643b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b80f7d9df2d14fcb9be52193bd2b434d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d9dd9aa74ca4e4d90f79b8de63689a4",
      "placeholder": "​",
      "style": "IPY_MODEL_66e45f80b8fc4e9082578c71b8218edf",
      "value": "Best trial: 48. Best value: 13225.8: 100%"
     }
    },
    "b9d988d43ba84b9593271d7ec904588b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c216d10d27684e579fc9f4b495024212": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6952b4430a524343beac9a8a3dd65ab3",
       "IPY_MODEL_6ed14a30ef644bbbbe5947d5d3e2ed2b",
       "IPY_MODEL_747d6c0e58a94884983518852af255c7"
      ],
      "layout": "IPY_MODEL_b9d988d43ba84b9593271d7ec904588b"
     }
    },
    "e912f0c09b2d46eb9158012b9f5bcc66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee8e7bfd4c6e4ea6989ce9310760845b",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab06fcac969542aab442b8446e573202",
      "value": 50
     }
    },
    "ee8e7bfd4c6e4ea6989ce9310760845b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f10ecbb062d744fd99e44d65f5f79829": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f86f0a77d97e41aaa0ac3083dddea4fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
